# Java-Is-All-You-Need
For final java project

1. [Overview](#overview)  
2. [Prerequisites](#prerequisites)  
3. [Directory Structure](#directory-structure)  
4. [Java Source Files](#java-source-files)  
5. [Running the Project](#running-the-project)  
   - [Build](#build)  
   - [Training Mode](#training-mode)  
   - [Generation Mode](#generation-mode)  
   - [Demo Main](#demo-main)  
6. [Testing](#testing)  
7. [Weights & .gitignore](#weights--gitignore)  
8. [Contributing](#contributing)  
9. [License](#license)  

---

## Overview

This project implements:

- A **Tokenizer** that builds a vocabulary from plain-text/JSON corpora and encodes/decodes.
- An **Embedding** layer storing token-embedding weights.
- A **Multi-Head Attention** module.
- A **Feed-Forward** sublayer.
- A **LayerNorm** module.
- A stacked **TransformerEncoderLayer**.
- A simple **LMHead** (linear projection to vocabulary logits).
- A **LanguageModel** wrapper to generate text autoregressively.
- A **CorpusTrainer** that reads a corpus, runs teacher-forcing training with a command-line progress bar and logging.
- A **GenAIApp** CLI with `train` and `gen` modes.
- A small **Main.java** demo illustrating one encoder layer.

---

## Prerequisites

- **Java 24** (JDK 24)  
- **Maven** 3.x  
- A UNIX-style shell for the provided bash menu script  

---

## Directory Structure
JavaIsAllYouNeed
â”œâ”€â”€ data
â”‚Â Â  â”œâ”€â”€ MediumTest
â”‚Â Â  â”‚Â Â  â””â”€â”€ 00c2bfc7-57db-496e-9d5c-d62f8d8119e3.json
â”‚Â Â  â””â”€â”€ TinyTest
â”‚Â Â      â””â”€â”€ 00c2bfc7-57db-496e-9d5c-d62f8d8119e3.json
â”œâ”€â”€ dependency-reduced-pom.xml
â”œâ”€â”€ main
â”œâ”€â”€ pom.xml
â”œâ”€â”€ src
â”‚Â Â  â”œâ”€â”€ main
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ java
â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ org
â”‚Â Â  â”‚Â Â  â”‚Â Â      â””â”€â”€ anjali
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ CorpusTrainer.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ Embedding.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ FeedForward.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ GenAIApp.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ LanguageModel.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ LayerNorm.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ Linear.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ LMHead.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ Main.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ MultiHeadAttention.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â”œâ”€â”€ Tokenizer.java
â”‚Â Â  â”‚Â Â  â”‚Â Â          â””â”€â”€ TransformerEncoderLayer.java
â”‚Â Â  â”‚Â Â  â””â”€â”€ resources
â”‚Â Â  â””â”€â”€ test
â”‚Â Â      â””â”€â”€ java
â”‚Â Â          â””â”€â”€ org
â”‚Â Â              â””â”€â”€ anjali
â”‚Â Â                  â””â”€â”€ TransformerEncoderLayerTest.java
â””â”€â”€ target
    â”œâ”€â”€ classes
    â”‚Â Â  â””â”€â”€ org
    â”‚Â Â      â””â”€â”€ anjali
    â”‚Â Â          â”œâ”€â”€ CorpusTrainer$Result.class
    â”‚Â Â          â”œâ”€â”€ CorpusTrainer.class
    â”‚Â Â          â”œâ”€â”€ Embedding.class
    â”‚Â Â          â”œâ”€â”€ FeedForward.class
    â”‚Â Â          â”œâ”€â”€ GenAIApp.class
    â”‚Â Â          â”œâ”€â”€ LanguageModel.class
    â”‚Â Â          â”œâ”€â”€ LayerNorm.class
    â”‚Â Â          â”œâ”€â”€ Linear.class
    â”‚Â Â          â”œâ”€â”€ LMHead.class
    â”‚Â Â          â”œâ”€â”€ Main.class
    â”‚Â Â          â”œâ”€â”€ MultiHeadAttention.class
    â”‚Â Â          â”œâ”€â”€ Tokenizer.class
    â”‚Â Â          â””â”€â”€ TransformerEncoderLayer.class
    â”œâ”€â”€ FinalProject2-1.0-SNAPSHOT.jar
    â”œâ”€â”€ generated-sources
    â”‚Â Â  â””â”€â”€ annotations
    â”œâ”€â”€ generated-test-sources
    â”‚Â Â  â””â”€â”€ test-annotations
    â”œâ”€â”€ maven-archiver
    â”‚Â Â  â””â”€â”€ pom.properties
    â”œâ”€â”€ maven-status
    â”‚Â Â  â””â”€â”€ maven-compiler-plugin
    â”‚Â Â      â”œâ”€â”€ compile
    â”‚Â Â      â”‚Â Â  â””â”€â”€ default-compile
    â”‚Â Â      â”‚Â Â      â”œâ”€â”€ createdFiles.lst
    â”‚Â Â      â”‚Â Â      â””â”€â”€ inputFiles.lst
    â”‚Â Â      â””â”€â”€ testCompile
    â”‚Â Â          â””â”€â”€ default-testCompile
    â”‚Â Â              â”œâ”€â”€ createdFiles.lst
    â”‚Â Â              â””â”€â”€ inputFiles.lst
    â”œâ”€â”€ original-FinalProject2-1.0-SNAPSHOT.jar
    â”œâ”€â”€ surefire-reports
    â”‚Â Â  â”œâ”€â”€ org.anjali.TransformerEncoderLayerTest.txt
    â”‚Â Â  â””â”€â”€ TEST-org.anjali.TransformerEncoderLayerTest.xml
    â””â”€â”€ test-classes
        â””â”€â”€ org
            â””â”€â”€ anjali
                â””â”€â”€ TransformerEncoderLayerTest.class

33 directories, 41 files

## Java Source Files

### 1. Overall Architecture  
![image](https://github.com/user-attachments/assets/752db052-0c4c-4bb1-ab3c-8a5ff486ffa8)

- **TransformerEncoderLayer.java** ties together self-attention, residuals & layer norms, and feed-forward blocks exactly as in Sections 3.1â€“3.4 of Vaswani et al. :contentReference[oaicite:2]{index=2}.

---

### 2. Scaled Dot-Product & Multi-Head Attention  
- **Section 3.2:**  
  > â€œScaled dot-product attentionâ€ computes  
  > \[ \text{Attention}(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}}) V \]  
  > Multi-head attention runs this in parallel heads. :contentReference[oaicite:3]{index=3}  
- **Mapping:**  
  - `MultiHeadAttention.java`  
    - Projects inputs into Q, K, V (via four `Linear` layers).  
    - Splits into `numHeads`, applies scaled dot-product, concatenates, and final projection.  

---

### 3. Position-Wise Feed-Forward Networks  
- **Section 3.3:**  
  > Two linear layers with ReLU in between (applied independently to each position). :contentReference[oaicite:4]{index=4}  
- **Mapping:**  
  - `FeedForward.java` implements exactly that:  
    1. `Linear(embedDimâ†’hiddenDim)` + ReLU  
    2. `Linear(hiddenDimâ†’embedDim)`

---

### 4. Residual Connections & Layer Normalization  
- **Section 3.1 & 3.4:**  
  > â€œEach sub-layer is surrounded by a residual connection followed by layer normalization.â€ :contentReference[oaicite:5]{index=5}  
- **Mapping:**  
  - `TransformerEncoderLayer.java` calls `add(...)` for residuals, then `LayerNorm.forward(...)`.  

---

### 5. Autoregressive Generation  
- **Greedy Decoding:**  
  > After training, we generate one token at a time by picking the highest-probability next token. This style of autoregressive generation builds on early seq2seq work :contentReference[oaicite:6]{index=6}.  
- **Mapping:**  
  - `LanguageModel.java`:  
    - `encodePrompt(...)` to get initial `[<BOS>, â€¦]`  
    - Loop until `<EOS>` or maxLen: embed â†’ encoder stack â†’ `LMHead.forward(...)` â†’ argmax â†’ append.  
    - `decodeAll(...)` converts IDs back to tokens.

---

### ðŸ”§ Java Source Files

Below is a quick reference for where each major Transformer component lives in the code:

| Paper Concept                                | Java Class                           |
|----------------------------------------------|--------------------------------------|
| Tokenization & Special Tokens                | `Tokenizer.java`                     |
| Embedding Look-Up                            | `Embedding.java`                     |
| Scaled Dot-Product + Multi-Head Attention    | `MultiHeadAttention.java`            |
| Position-Wise Feed-Forward                   | `FeedForward.java`                   |
| Layer Normalization                          | `LayerNorm.java`                     |
| Residual + Sublayer Composition              | `TransformerEncoderLayer.java`       |
| Final LM Head Projection                     | `LMHead.java`                        |
| Full Model + Greedy Generation               | `LanguageModel.java`                 |
| Training Loop / Teacher-Forcing + Progress   | `CorpusTrainer.java`                 |
| CLI Entry-Point (`train` / `gen` modes)      | `GenAIApp.java`                      |
| Minimal demo of a single encoder layer       | `Main.java`                          |


---


